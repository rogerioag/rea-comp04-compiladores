{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-comp-analise-lexica-cmmlex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerioag/rea-comp04-compiladores/blob/main/jupyter-notebooks/01-comp-analise-lexica-cmmlex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ai_mHoMeYJV"
      },
      "source": [
        "# Análise Léxica\n",
        "\n",
        "A __Análise Léxica__ é a fase do compilador que lê o código-fonte do arquivo de entrada como um fluxo de caracteres, e nesse processo de varredura reconhece os _tokens_ ou marcas da linguagem. As denominações Sistema de Varredura, Analisador Léxico e _Scanner_ são equivalentes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuQK8K5v_UHV"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Devem ser reconhecidas as marcas presentes na linguagem `C-`, como `if`, `while` que são palavras chave, palavras reservadas. Precisam ser reconhecidos os nomes de variáveis e funções que são os _identificadores_, símbolos e operadores aritméticos, lógicos e relacionais.\n",
        "\n",
        "O processo de reconhecimento das marcas, a identificação de padrões pode ser feito de duas formas: utilizando-se __expressões regulares__ ou implementando o analisador com a teoria de __autômatos finitos__.\n",
        "\n",
        "Neste tutorial iremos utilizar _expressões regulares_ para a especificarmos os padrões de reconhecimentos das marcas.\n",
        "\n",
        "Para um código simples em `C-` igual o Código 1.\n",
        "\n",
        "```c\n",
        "/* Programa simples. */\n",
        "\n",
        "int main(void){\n",
        "  return(0);\n",
        "}\n",
        "```\n",
        "_Código 1: Programa em C-_\n",
        "\n",
        "\n",
        "A lista de marcas que precisam ser identificadas é:\n",
        "```\n",
        "INT\n",
        "ID\n",
        "LPAREN\n",
        "VOID\n",
        "RPAREN\n",
        "LBRACES\n",
        "RETURN\n",
        "LPAREN\n",
        "NUMBER\n",
        "RPAREN\n",
        "SEMICOLON\n",
        "RBRACES\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK4ILCH4_Y3X"
      },
      "source": [
        "## Leitura Recomendada\n",
        "\n",
        "1. __Capítulo 2:__ _Varredura_\n",
        "\n",
        "    LOUDEN, Kenneth C. Compiladores: princípios e práticas. São Paulo, SP: Thomson, c2004. xiv, 569 p. ISBN 8522104220.\n",
        "\n",
        "2. __Capítulo 3:__ _Análise Léxica_\n",
        "\n",
        "    AHO, Alfred V. et al. Compiladores: princípios, técnicas e ferramentas. 2. ed. São Paulo, SP: Pearson Addison-Wesley, 2008. x, 634 p. ISBN 9788588639249."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V-XYl_H_JBV"
      },
      "source": [
        "## Preparação do Ambiente\n",
        "\n",
        "Para a implementação da Análise Léxica é necessário instalar a ferramentas `PLY`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9TX9STdBDfD",
        "outputId": "932e75a5-34a8-4492-f882-6b55a182e609"
      },
      "source": [
        "!pip install ply\n",
        "!pip install anytree\n",
        "!pip install graphviz\n",
        "!pip install llvmlite\n",
        "!jupyter nbextension install https://rawgit.com/jfbercher/small_nbextensions/master/highlighter.zip  --user\n",
        "!jupyter nbextension enable highlighter/highlighter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ply in /usr/local/lib/python3.7/dist-packages (3.11)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from anytree) (1.15.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: llvmlite in /usr/local/lib/python3.7/dist-packages (0.34.0)\n",
            "Downloading: https://rawgit.com/jfbercher/small_nbextensions/master/highlighter.zip -> /tmp/tmprfYPBo/highlighter.zip\n",
            "Extracting: /tmp/tmprfYPBo/highlighter.zip -> /root/.local/share/jupyter/nbextensions\n",
            "Enabling notebook extension highlighter/highlighter...\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNT8lJABWX9l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "80910df2-96df-44b2-f551-8d6ef2a17ef7"
      },
      "source": [
        "%%javascript\n",
        "require(\"base/js/utils\").load_extensions(\"highlighter/highlighter\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "require(\"base/js/utils\").load_extensions(\"highlighter/highlighter\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5zbHzlP92i7"
      },
      "source": [
        "## Copiando Módulos do Projeto do Github\n",
        "\n",
        "Para a realização de testes com o código de referência (código de _start_) podemos baixar os arquivos do projeto `rea-comp04-compiladores` do repositório do [`github`](https://github.com/rogerioag/rea-comp04-compiladores):\n",
        "\n",
        "```bash\n",
        "! git clone https://github.com/rogerioag/rea-comp04-compiladores.git\n",
        "```\n",
        "Comandos do `terminal` podem ser executados sendo precedidos de `!`.\n",
        "\n",
        "Iremos copiar os módulos e os arquivos de testes para o diretório de conteúdo do _notebook_, o __content__.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nDqdVEX2Pk1",
        "outputId": "ee71fba6-7fc0-49d6-e8c1-d333d6d095bd"
      },
      "source": [
        "! rm -rf rea-comp04-compiladores\n",
        "! git clone https://github.com/rogerioag/rea-comp04-compiladores.git\n",
        "! cp -R rea-comp04-compiladores/cmmcompiler/* .\n",
        "! cp -R rea-comp04-compiladores/cmmcompiler/tests/* .\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rea-comp04-compiladores'...\n",
            "remote: Enumerating objects: 474, done.\u001b[K\n",
            "remote: Counting objects: 100% (474/474), done.\u001b[K\n",
            "remote: Compressing objects: 100% (378/378), done.\u001b[K\n",
            "remote: Total 474 (delta 268), reused 228 (delta 85), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (474/474), 2.87 MiB | 4.28 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYI9B4jGCH5x"
      },
      "source": [
        "## Código de Referência\n",
        "\n",
        "O código do projeto `cmmcompiler` foi copiado para o servidor do `Colab`.\n",
        "Utilizando `%%writefile` é possível gravar o conteúdo da célula do _notebook_ em um arquivo do lado servidor do `Colab`, inclusive alterar o conteúdo original dos arquivos do projeto.\n",
        "\n",
        "O arquivo `lexer/tokens.py` faz definição das palavras reservadas, símbolos matemáticos como operadores aritméticos, operadores relacionais, símbolos de controle, identificadores, e da lista de _tokens_: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAOrk6zIm3Gr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a409889-ee68-4917-b3dc-389f2d9c3ad4"
      },
      "source": [
        "%%writefile lexer/tokens.py\n",
        "\n",
        "__all__ = ['tokens', 'TOKENS_SYMBOLS']\n",
        "\n",
        "reserved = {\n",
        "    'else': 'ELSE',\n",
        "    'if': 'IF',\n",
        "    'int': 'INT',\n",
        "    'return': 'RETURN',\n",
        "    'void': 'VOID',\n",
        "    'while': 'WHILE',\n",
        "}\n",
        "\n",
        "math_symbols = [\n",
        "    'PLUS',  # +\n",
        "    'MINUS',  # -\n",
        "    'TIMES',  # *\n",
        "    'DIVIDE'  # /\n",
        "]\n",
        "\n",
        "comparison_symbols = [\n",
        "    'LESS_EQUAL',  # <=\n",
        "    'LESS',  # <\n",
        "    'GREATER_EQUAL',  # >=\n",
        "    'GREATER',  # >\n",
        "    'EQUALS',  # ==\n",
        "    'DIFFERENT',  # !=\n",
        "]\n",
        "\n",
        "control_symbols = [\n",
        "    'LPAREN',  # (\n",
        "    'RPAREN',  # )\n",
        "    'LBRACKETS',  # [\n",
        "    'RBRACKETS',  # ]\n",
        "    'LBRACES',  # {\n",
        "    'RBRACES',  # }\n",
        "    'ATTRIBUTION',  # =\n",
        "    'SEMICOLON',  # ;\n",
        "    'COMMA',  # ,\n",
        "]\n",
        "\n",
        "markers = [\n",
        "    'ID',\n",
        "    'NUMBER',\n",
        "]\n",
        "\n",
        "TOKENS_SYMBOLS = {\n",
        "    'PLUS': '+',\n",
        "    'MINUS': '-',\n",
        "    'TIMES': '*',\n",
        "    'DIVIDE': '/',\n",
        "    'LESS_EQUAL': '<=',\n",
        "    'LESS': '<',\n",
        "    'GREATER_EQUAL': '>=',\n",
        "    'GREATER': '>',\n",
        "    'EQUALS': '==',\n",
        "    'DIFFERENT': '!=',\n",
        "    'LPAREN': '(',\n",
        "    'RPAREN': ')',\n",
        "    'LBRACKETS': '[',\n",
        "    'RBRACKETS': ']',\n",
        "    'LBRACES': '{',\n",
        "    'RBRACES': '}',\n",
        "    'ATTRIBUTION': '=',\n",
        "    'SEMICOLON': ';',\n",
        "    'COMMA': ',',\n",
        "    'ELSE': 'else',\n",
        "    'IF': 'if',\n",
        "    'INT': 'int',\n",
        "    'RETURN': 'return',\n",
        "    'VOID': 'void',\n",
        "    'WHILE': 'while',\n",
        "}\n",
        "\n",
        "tokens = markers + math_symbols + comparison_symbols + \\\n",
        "    control_symbols + list(reserved.values())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lexer/tokens.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zONixqpZC4uK"
      },
      "source": [
        "Definição das Expressões Regulares para cada uma das classes de _tokens_: `lexer/regexs.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_nkB54m_-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc49ff7-2093-4659-a251-916e93f7c617"
      },
      "source": [
        "%%writefile lexer/regexs.py\n",
        "\n",
        "id_regex = r'[a-zA-Z][a-zA-Z]*'\n",
        "comment_regex = r'\\/\\*[^\\r]*\\*\\/'\n",
        "\n",
        "#! MATH\n",
        "t_PLUS = r'\\+'\n",
        "t_MINUS = r'-'\n",
        "t_TIMES = r'\\*'\n",
        "t_DIVIDE = r'/'\n",
        "\n",
        "#! COMPARISON\n",
        "t_LESS_EQUAL = r'<='\n",
        "t_LESS = r'<'\n",
        "t_GREATER_EQUAL = r'>='\n",
        "t_GREATER = r'>'\n",
        "t_EQUALS = r'=='\n",
        "t_DIFFERENT = r'!='\n",
        "\n",
        "#! CONTROL\n",
        "t_LPAREN = r'\\('\n",
        "t_RPAREN = r'\\)'\n",
        "t_LBRACKETS = r'\\['\n",
        "t_RBRACKETS = r'\\]'\n",
        "t_LBRACES = r'{'\n",
        "t_RBRACES = r'}'\n",
        "t_ATTRIBUTION = r'='\n",
        "t_SEMICOLON = r';'\n",
        "t_COMMA = r','\n",
        "\n",
        "t_NUMBER = r'[0-9][0-9]*'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lexer/regexs.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUnbtl9JDE4F"
      },
      "source": [
        "Definição das funções que representam as ações para cada classe de _token_ reconhecida: `lexer/methods.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIvcrpB_nJn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d90087-01e0-4552-880b-105966b1cb77"
      },
      "source": [
        "%%writefile lexer/methods.py\n",
        "\n",
        "from ply.lex import TOKEN\n",
        "\n",
        "from .regexs import id_regex, comment_regex\n",
        "from .tokens import reserved\n",
        "\n",
        "t_ignore = ' \\t'\n",
        "\n",
        "@TOKEN(id_regex)\n",
        "def t_ID(t):\n",
        "    t.type = reserved.get(t.value, 'ID')\n",
        "    return t\n",
        "\n",
        "@TOKEN(comment_regex)\n",
        "def t_ignore_COMMENT(r):\n",
        "    pass\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)\n",
        "\n",
        "def t_error(t):\n",
        "    print(\"Símbolo não definido pela linguagem '%s'\" % t.value[0])\n",
        "    t.lexer.skip(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lexer/methods.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXBdCCYdDSnB"
      },
      "source": [
        "Código principal do módulo `lexer`, instancia o `lexer` e define a função que devolve uma lista de _tokens_: `lexer/__init__.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4n4IwfrnZWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50997b6f-20c6-4aa0-ba20-dfb3a5979aba"
      },
      "source": [
        "%%writefile lexer/__init__.py\n",
        "\n",
        "__all__ = ['tokens', 'lexer', 'get_tokens', 'TOKENS_SYMBOLS', 'number_regex']\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# tokenizer for a C subset language called C-\n",
        "# ------------------------------------------------------------\n",
        "import ply.lex as lex\n",
        "from ply.lex import TOKEN\n",
        "\n",
        "from .tokens import *\n",
        "from .regexs import *\n",
        "from .methods import *\n",
        "\n",
        "from .regexs import t_NUMBER as number_regex\n",
        "\n",
        "from sys import argv, exit\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "     level = logging.DEBUG,\n",
        "     filename = \"cmmcompiler.log\",\n",
        "     filemode = \"w\",\n",
        "     format = \"%(filename)10s:%(lineno)4d:%(message)s\"\n",
        ")\n",
        "log = logging.getLogger()\n",
        "\n",
        "def get_tokens(input):\n",
        "    lexer = lex.lex()\n",
        "    lexer.input(input)\n",
        "\n",
        "    tokens = []\n",
        "    token = lexer.token()\n",
        "    while token:\n",
        "        tokens.append(token)\n",
        "        token = lexer.token()\n",
        "        pass\n",
        "\n",
        "    return tokens\n",
        "    pass\n",
        "\n",
        "\n",
        "lexer = lex.lex(optimize=True,debug=True,debuglog=log)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lexer/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmUmAgFSDulW"
      },
      "source": [
        "Código principal `cmmcompiler` que executa o módulo `lexer` e as outras fases do compilador: `main.py`.\n",
        "\n",
        "Nesta fase de Compilação será utilizada a função:\n",
        "\n",
        "```python\n",
        "def execute_lexical_analysis(source_input):\n",
        "    log.info(\"[lexer]: Executing Lexical Analysis.\")\n",
        "    for token in get_tokens(source_input):\n",
        "        print(token.type, token.value)\n",
        "    return\n",
        "```\n",
        "\n",
        "Esta função será chamada quando utilizarmos a opção `-l`com o `main.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4_q5L1Nsq2p",
        "outputId": "e2083c5f-24a1-4e9c-b74e-531d6b5a4e51"
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import utils\n",
        "from lexer import get_tokens\n",
        "from parser import parser\n",
        "from semantic import sema, Semantic\n",
        "from gencode import gencode, GenCode\n",
        "from tree import TreeNode\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    filename = \"cmmcompiler.log\",\n",
        "    encoding='utf-8',\n",
        "    level = logging.DEBUG,\n",
        "    filemode = \"w\",\n",
        "    format = \"%(filename)10s:%(lineno)4d:%(message)s\")\n",
        "log = logging.getLogger()\n",
        "\n",
        "syntax_tree = None\n",
        "reduced_syntax_tree = None\n",
        "\n",
        "def execute_lexical_analysis(source_input):\n",
        "    log.info(\"[lexer]: Executing Lexical Analysis.\")\n",
        "    for token in get_tokens(source_input):\n",
        "        print(token.type, token.value)\n",
        "    return\n",
        "\n",
        "def execute_syntax_analisys(source_input):\n",
        "    log.info(\"[parser]: Executing Syntax Analysis.\")\n",
        "    syntax_tree = parser.parse(source_input)\n",
        "    if syntax_tree != ():\n",
        "        print(\"Generating Syntax Tree Graph...\")\n",
        "        graph = utils.Graph(utils.args.file, 'Sintax Tree')\n",
        "        # program = parser.parse(source_input)\n",
        "        syntax_tree.render(graph)\n",
        "        graph.export()\n",
        "    return syntax_tree\n",
        "\n",
        "def execute_semantic_analisys(syntax_tree):\n",
        "    log.info(\"[sema]: Executing Semantic Analysis.\")\n",
        "    sema = Semantic(syntax_tree)\n",
        "    sema.check_semantic_rules()\n",
        "    return reduced_syntax_tree\n",
        "\n",
        "def execute_code_generation(reduced_syntax_tree):\n",
        "    log.info(\"[gencode]: Executing Code Generation.\")\n",
        "    gencode = GenCode(reduced_syntax_tree)\n",
        "    gencode.generate()\n",
        "    return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  with open(utils.args.file) as file:\n",
        "      source_input = file.read()\n",
        "      if utils.args.lexer:\n",
        "          execute_lexical_analysis(source_input)    \n",
        "      pass\n",
        "\n",
        "      if utils.args.parser:\n",
        "          syntax_tree = execute_syntax_analisys(source_input) \n",
        "      pass\n",
        "        \n",
        "      if utils.args.semantic:\n",
        "          if syntax_tree != ():\n",
        "              reduced_syntax_tree = execute_semantic_analisys(syntax_tree)\n",
        "          else:\n",
        "              syntax_tree = execute_syntax_analisys(source_input)\n",
        "              reduced_syntax_tree = execute_semantic_analisys(syntax_tree)\n",
        "          pass\n",
        "      pass\n",
        "\n",
        "      if utils.args.gencode:\n",
        "          if reduced_syntax_tree != None:\n",
        "              execute_code_generation(reduced_syntax_tree)\n",
        "          else:\n",
        "              syntax_tree = execute_syntax_analisys(source_input)\n",
        "              if syntax_tree != ():\n",
        "                  reduced_syntax_tree = execute_semantic_analisys(syntax_tree)\n",
        "                  execute_code_generation(reduced_syntax_tree)\n",
        "              pass\n",
        "\n",
        "      pass\n",
        "        \n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psIuDGVuD5gb"
      },
      "source": [
        "## Testes de outros exemplos\n",
        "\n",
        "Alguns arquivos de testes foram definidos no projeto. Utilizando `%%writefile` é possível gravar o conteúdo da célula do _notebook_ em um arquivo do lado servidor do Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efpzLNvTpZCP",
        "outputId": "079b302c-75a2-445e-99c2-4f917f3917b0"
      },
      "source": [
        "%%writefile prog-001.cm\n",
        "\n",
        "int main(){\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting prog-001.cm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgpJmj18oO18",
        "outputId": "8129bb67-5a0f-4698-e51e-840eb80c343f"
      },
      "source": [
        "%%writefile prog-002.cm\n",
        "\n",
        "int main(){\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing prog-002.cm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me5uEfceFRFR"
      },
      "source": [
        "A implementação do _Analisador Léxico_ pode ser chamada a partir do código principal `main.py` com o parâmetro `-l`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6geznynhxI0t",
        "outputId": "1bcc7752-eeb3-4b76-8dcd-c52cf7f45eb9"
      },
      "source": [
        "! python main.py -l prog-001.cm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating LALR tables\n",
            "WARNING: 1 shift/reduce conflict\n",
            "INT int\n",
            "ID main\n",
            "LPAREN (\n",
            "VOID void\n",
            "RPAREN )\n",
            "LBRACES {\n",
            "RBRACES }\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH-N4tXNhxlR"
      },
      "source": [
        "## Projeto de Implementação\n",
        "\n",
        "A gramática da linguagem `C-` tem suporte somente para os tipos `int` e `void`. Como Trabalho de Implementação vinculado à _Análise Léxica_, além da implementação do `Analisador Léxico` pode ser solicitado aos alunos que implementem o suporte ao tipo `float`, para que seja possível trabalhar com variáveis do tipo ponto flutuante.\n",
        "\n",
        "Que se daria pela inclusão de _float_ na lista de palavras _reservadas_:\n",
        "\n",
        "```c\n",
        "reserved = {\n",
        "    'else': 'ELSE',\n",
        "    'if': 'IF',\n",
        "    'int': 'INT',\n",
        "    'float': 'FLOAT', // Adição do float.\n",
        "    'return': 'RETURN',\n",
        "    'void': 'VOID',\n",
        "    'while': 'WHILE',\n",
        "}\n",
        "```\n",
        "\n",
        "E pela adição do _token_ para _float_ na lista de _tokens_:\n",
        "\n",
        "```c\n",
        "TOKENS_SYMBOLS = {\n",
        "    'PLUS': '+',\n",
        "    'MINUS': '-',\n",
        "    'TIMES': '*',\n",
        "    'DIVIDE': '/',\n",
        "    'LESS_EQUAL': '<=',\n",
        "    'LESS': '<',\n",
        "    'GREATER_EQUAL': '>=',\n",
        "    'GREATER': '>',\n",
        "    'EQUALS': '==',\n",
        "    'DIFFERENT': '!=',\n",
        "    'LPAREN': '(',\n",
        "    'RPAREN': ')',\n",
        "    'LBRACKETS': '[',\n",
        "    'RBRACKETS': ']',\n",
        "    'LBRACES': '{',\n",
        "    'RBRACES': '}',\n",
        "    'ATTRIBUTION': '=',\n",
        "    'SEMICOLON': ';',\n",
        "    'COMMA': ',',\n",
        "    'ELSE': 'else',\n",
        "    'IF': 'if',\n",
        "    'INT': 'int',\n",
        "    'FLOAT': 'float', // Adição do Token FLOAT.\n",
        "    'RETURN': 'return',\n",
        "    'VOID': 'void',\n",
        "    'WHILE': 'while',\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eyzSr_yKUO6"
      },
      "source": [
        "## Referências\n",
        "\n",
        "[1] LOUDEN, Kenneth C. **Compiladores:** princípios e práticas. São Paulo, SP: Thomson, c2004. xiv, 569 p. ISBN 8522104220.\n",
        "\n",
        "[2] AHO, Alfred V. **Compiladores:** princípios, técnicas e ferramentas. 2. ed. São Paulo: Pearson Addison-Wesley, 2008. x, 634 p. ISBN 9788588639249."
      ]
    }
  ]
}